# üï∑Ô∏è Site Crawler Feature

Your web scraper now includes a powerful site crawler that can analyze entire websites to identify systemic SEO issues. This feature is perfect for comprehensive SEO audits and identifying patterns across multiple pages.

## ‚úÖ **What's Been Added**

### üîß **Core Functionality**
- **Intelligent URL Discovery** - Finds URLs via sitemaps and link crawling
- **Batch SEO Analysis** - Analyzes multiple pages automatically
- **Issue Detection** - Identifies 15+ types of SEO problems
- **Systemic Analysis** - Highlights patterns and common issues
- **Detailed Reporting** - Comprehensive results with filtering and export

### üéØ **SEO Issues Detected**

#### **Critical Issues**
- Missing title tags
- Empty title tags
- Missing H1 tags
- Empty H1 tags

#### **High Priority Issues**
- Missing meta descriptions
- Not mobile-friendly pages
- Slow page load times (>3 seconds)

#### **Medium Priority Issues**
- Title too short (<30 chars) or too long (>60 chars)
- Meta description too short (<120 chars) or too long (>160 chars)
- Multiple H1 tags on same page

#### **Content & Structure Issues**
- Low content volume (<300 words)
- No H2 subheadings
- Too many H2 tags (>10)
- Images missing alt text
- Generic titles ("Welcome to", "Untitled")

#### **Performance Issues**
- Page load times >2 seconds (warning) or >3 seconds (critical)
- Very long content (>3000 words)

## üöÄ **How to Use**

### **1. Start a New Crawl**
1. Go to **Site Crawler** in the navigation
2. Enter the website URL you want to crawl
3. Configure crawl settings:
   - **Max URLs**: 25, 50, 100, or 200 pages
   - **Max Depth**: 1-5 levels deep from starting page
   - **Robots.txt**: Option to ignore for research purposes

### **2. Crawl Settings Explained**
- **Max URLs**: Total number of pages to analyze (more = longer crawl time)
- **Max Depth**: How many "clicks" away from the starting page to go
- **Depth 1**: Only the starting page
- **Depth 2**: Starting page + all pages it links to
- **Depth 3**: Previous + all pages those pages link to

### **3. View Results**
- **Summary Dashboard**: Overview of total issues, categories, severity
- **Detailed Table**: Every page with specific issues identified
- **Filtering**: View all pages, only pages with issues, or critical issues
- **Export**: Download results as CSV or JSON

## üìä **Results Analysis**

### **Summary Statistics**
- **Total Pages Analyzed**
- **Total Issues Found**
- **Percentage of Pages with Issues**
- **Average Issues per Page**

### **Issue Categorization**
Issues are automatically categorized by:
- **Type**: Title, Meta Description, Headers, Performance, Mobile, Content, Images
- **Severity**: Critical, High, Medium, Low

### **Common Issues Report**
- Lists the most frequently occurring issues across all pages
- Helps identify systemic problems affecting multiple pages

## üéõÔ∏è **Advanced Features**

### **Intelligent URL Discovery**
1. **Sitemap Parsing**: Automatically finds and parses sitemap.xml
2. **Link Following**: Discovers URLs by following internal links
3. **Robots.txt Integration**: Respects or ignores robots.txt as configured
4. **Duplicate Prevention**: Avoids analyzing the same URL twice

### **Respectful Crawling**
- **Built-in Delays**: 0.5 second delay between requests
- **Timeout Handling**: 30-second timeout per page
- **Error Recovery**: Continues crawling even if some pages fail
- **Rate Limiting**: Prevents overwhelming target servers

### **Progress Tracking**
- **Real-time Progress**: Shows crawl progress as it happens
- **Session Management**: All crawls are saved with timestamps
- **Status Tracking**: Pending, Running, Completed, or Failed status

## üîç **Use Cases**

### **SEO Audits**
- **New Website Launch**: Ensure all pages meet SEO standards
- **Periodic Reviews**: Regular checks for SEO compliance
- **Migration Audits**: Verify SEO after site redesigns
- **Competitor Analysis**: Analyze competitor sites for insights

### **Content Strategy**
- **Content Gaps**: Find pages with insufficient content
- **Title Optimization**: Identify pages needing better titles
- **Meta Description Audit**: Ensure all pages have compelling descriptions
- **Header Structure**: Verify proper H1/H2 hierarchy

### **Technical SEO**
- **Performance Issues**: Find slow-loading pages
- **Mobile Compliance**: Ensure all pages are mobile-friendly
- **Accessibility**: Check for missing alt text on images
- **Structural Problems**: Identify pages with poor HTML structure

## üìà **Best Practices**

### **Crawl Configuration**
- **Start Small**: Begin with 25-50 URLs for initial testing
- **Increase Gradually**: Use larger limits for comprehensive audits
- **Depth Strategy**: Use depth 2-3 for most sites
- **Respect Limits**: Don't overwhelm small sites with large crawls

### **Results Analysis**
- **Focus on Critical Issues First**: Address missing titles and H1s
- **Look for Patterns**: Identify issues affecting multiple pages
- **Prioritize High-Traffic Pages**: Fix issues on important pages first
- **Track Progress**: Re-crawl periodically to measure improvements

### **Ethical Considerations**
- **Respect robots.txt**: Only ignore for legitimate research
- **Be Considerate**: Don't crawl the same site repeatedly
- **Check Terms of Service**: Ensure crawling is permitted
- **Use Responsibly**: This tool is for analysis, not data harvesting

## üõ†Ô∏è **Technical Details**

### **Crawl Process**
1. **URL Discovery**: Parse sitemap.xml and extract links
2. **Queue Management**: Organize URLs by depth level
3. **Batch Analysis**: Analyze each URL for SEO issues
4. **Issue Detection**: Apply 15+ SEO rules to each page
5. **Data Storage**: Save results to TinyDB for later analysis

### **Performance**
- **Crawl Speed**: ~1-2 seconds per URL (including analysis)
- **Memory Usage**: Efficient for sites up to 200 pages
- **Storage**: Results stored in human-readable JSON format
- **Concurrent Processing**: Multi-threaded for faster crawling

### **Error Handling**
- **Network Errors**: Graceful handling of timeouts and connection issues
- **Parse Errors**: Continues crawling even if some pages can't be parsed
- **Robots.txt Errors**: Assumes allowed if robots.txt can't be read
- **Partial Results**: Saves successful analyses even if some pages fail

## üìù **API Usage**

### **Start Crawl**
```bash
curl -X POST http://localhost:5000/api/crawl-site \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "max_urls": 50,
    "max_depth": 2,
    "respect_robots": true
  }'
```

### **Export Results**
```bash
# CSV Export
curl "http://localhost:5000/api/export/crawl-results/SESSION_ID?format=csv"

# JSON Export  
curl "http://localhost:5000/api/export/crawl-results/SESSION_ID?format=json"
```

## üéâ **Benefits**

### **For SEO Professionals**
- **Comprehensive Audits**: Analyze entire sites quickly
- **Issue Prioritization**: Focus on critical problems first
- **Progress Tracking**: Monitor improvements over time
- **Client Reporting**: Export professional reports

### **For Developers**
- **Pre-Launch Checks**: Ensure SEO compliance before going live
- **Regression Testing**: Verify SEO after code changes
- **Performance Monitoring**: Track page speed across site
- **Accessibility Audits**: Find missing alt text and other issues

### **For Content Teams**
- **Content Quality**: Identify pages needing better content
- **Title Optimization**: Find pages with poor titles
- **Meta Description Gaps**: Ensure all pages have descriptions
- **Structure Analysis**: Verify proper heading hierarchy

---

**Your site crawler is now ready to help you identify and fix systemic SEO issues across entire websites!** üöÄ

This powerful feature transforms your web scraper from a single-page analysis tool into a comprehensive SEO audit platform.
